{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T01:10:48.506625Z",
     "iopub.status.busy": "2022-04-25T01:10:48.50635Z",
     "iopub.status.idle": "2022-04-25T01:11:23.37502Z",
     "shell.execute_reply": "2022-04-25T01:11:23.374268Z",
     "shell.execute_reply.started": "2022-04-25T01:10:48.506586Z"
    }
   },
   "outputs": [],
   "source": [
    "# download test data\n",
    "!wget -O GTZAN_TEST.zip 'https://www.aliyundrive.com/drive/folder/6389d74426d2a0b9fba54e67bc7778c7f299e6d9/GTZAN_TEST.zip' --no-check-certificate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip\n",
    "!unzip GTZAN_TEST.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T01:12:54.398331Z",
     "iopub.status.busy": "2022-04-25T01:12:54.398021Z",
     "iopub.status.idle": "2022-04-25T01:12:54.406079Z",
     "shell.execute_reply": "2022-04-25T01:12:54.40533Z",
     "shell.execute_reply.started": "2022-04-25T01:12:54.398296Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "from itertools import product\n",
    "from collections import namedtuple\n",
    "from collections import OrderedDict\n",
    "from IPython.display import display,clear_output\n",
    "import time\n",
    "import json\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_printoptions(linewidth=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T01:12:58.357786Z",
     "iopub.status.busy": "2022-04-25T01:12:58.357318Z",
     "iopub.status.idle": "2022-04-25T01:12:58.467894Z",
     "shell.execute_reply": "2022-04-25T01:12:58.467203Z",
     "shell.execute_reply.started": "2022-04-25T01:12:58.357642Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add a column to the alphabetical list of tag styles in the tag file in numeric format\n",
    "# test dataset\n",
    "ANNOTATIONS_FILE = \"./GTZAN_TEST/features_30_sec_test.csv\"\n",
    "dataframe = pd.read_csv(ANNOTATIONS_FILE)\n",
    "\n",
    "labels = set()\n",
    "for row in range(len(dataframe)):\n",
    "    labels.add(dataframe.iloc[row, -1])\n",
    "labels_list = []\n",
    "for label in labels:\n",
    "    labels_list.append(label)\n",
    "sorted_labels = sorted(labels_list)\n",
    "sorted_labels\n",
    "mapping = {}\n",
    "for index, label in enumerate(sorted_labels):\n",
    "    mapping[label] = index\n",
    "dataframe[\"num_label\"] = dataframe[\"label\"]\n",
    "new_dataframe = dataframe.replace({\"num_label\": mapping})\n",
    "new_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T01:13:02.525986Z",
     "iopub.status.busy": "2022-04-25T01:13:02.525664Z",
     "iopub.status.idle": "2022-04-25T01:13:02.5594Z",
     "shell.execute_reply": "2022-04-25T01:13:02.558793Z",
     "shell.execute_reply.started": "2022-04-25T01:13:02.52595Z"
    }
   },
   "outputs": [],
   "source": [
    "new_dataframe.to_csv(\"features_30_sec_test_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T01:13:11.084Z",
     "iopub.status.busy": "2022-04-25T01:13:11.083361Z",
     "iopub.status.idle": "2022-04-25T01:13:15.391828Z",
     "shell.execute_reply": "2022-04-25T01:13:15.390077Z",
     "shell.execute_reply.started": "2022-04-25T01:13:11.083958Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data pre-processing class\n",
    "# \n",
    "class GTZANDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 annotations_file,\n",
    "                 audio_dir,\n",
    "                 transformation,\n",
    "                 target_sample_rate,\n",
    "                 num_samples,\n",
    "                 device):\n",
    "        # Load tag files\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        # Load audio address\n",
    "        self.audio_dir = audio_dir\n",
    "        # set the device\n",
    "        self.device = device\n",
    "        # Merle spectrum data loaded into the device\n",
    "        self.transformation = transformation.to(self.device)\n",
    "        # Set sampling rate\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        # Set the number of samples\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "        \n",
    "    # Returns how many audio files there are\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    \n",
    "    # The data, tags, and paths of the audio can be obtained by means of arrays\n",
    "    def __getitem__(self, index):\n",
    "        # Get song path\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        # get tag\n",
    "        label = self._get_audio_sample_label(index)\n",
    "        # signal  sr : rate\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        signal = signal.to(self.device)\n",
    "        # control\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        # Dual channel->single channel\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        # Control the number of samples\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        # Transformation of mel spectrum\n",
    "        signal = self.transformation(signal)\n",
    "        return signal, label, audio_sample_path\n",
    "\n",
    "    \n",
    "    # Whether the signal should be cropped: If the number of picks > the set number -> cropping\n",
    "    def _cut_if_necessary(self, signal):\n",
    "        # print('_cut_if_necessary')\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "    \n",
    "    \n",
    "    # Whether the signal needs to be replenished: fill in 0 to the right to replenish, if the number of picking < the set number -> replenish\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        # print('_right_pad_if_necessary')\n",
    "        if length_signal < self.num_samples:\n",
    "            \n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            # last_dim_padding.to(self.device)\n",
    "            \n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "\n",
    "        return signal\n",
    "\n",
    "    \n",
    "    # Resetting the sampling frequency\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        # print('_resample_if_necessary')\n",
    "        # If the actual sampling frequency is not the same as the set one, then only reset\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate).to(self.device)\n",
    "            signal = resampler(signal)\n",
    "            # signal = torchaudio.functional.resample(signal, sr, self.target_sample_rate)\n",
    "            \n",
    "        return signal\n",
    "\n",
    "\n",
    "    # Change the dual channel of audio to single channel\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        # print('_mix_down_if_necessary')\n",
    "        # If the number of channels is greater than 1, the average value becomes a single channel.\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "\n",
    "    # Splicing and extraction of audio paths\n",
    "    def _get_audio_sample_path(self, index):\n",
    "        # print('_get_audio_sample_path')\n",
    "        fold = f\"{self.annotations.iloc[index, -2]}\"\n",
    "        path = os.path.join(self.audio_dir, fold, self.annotations.iloc[\n",
    "            index, 1])\n",
    "        return path\n",
    "    \n",
    "    \n",
    "    # ä»ŽExtracting tags from csv files\n",
    "    def _get_audio_sample_label(self, index):\n",
    "        # print('_get_audio_sample_label')\n",
    "        return self.annotations.iloc[index, -1]\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ANNOTATIONS_FILE = \"./features_30_sec_final.csv\"\n",
    "    AUDIO_DIR = \"./GTZAN/genres_original\"\n",
    "    SAMPLE_RATE = 22050\n",
    "    NUM_SAMPLES = 22050 * 5 # -> 1 second of audio\n",
    "    plot = True\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    print(f\"Using {device} device\")\n",
    "\n",
    "    mfcc = torchaudio.transforms.MFCC(\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        n_mfcc=40,\n",
    "        log_mels=True\n",
    "    )\n",
    "\n",
    "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        n_fft=1024,\n",
    "        # Window size\n",
    "        hop_length=512,\n",
    "        # Mel Frequency\n",
    "        n_mels=64\n",
    "    )\n",
    "\n",
    "    # objects inside transforms module are callable!\n",
    "    # ms = mel_spectrogram(signal)\n",
    "\n",
    "    gtzan = GTZANDataset(\n",
    "        ANNOTATIONS_FILE,\n",
    "        AUDIO_DIR,\n",
    "        mfcc,\n",
    "        SAMPLE_RATE,\n",
    "        NUM_SAMPLES,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    print(f\"There are {len(gtzan)} samples in the dataset\")\n",
    "\n",
    "    if plot:\n",
    "        signal, label, path = gtzan[666]\n",
    "        print(f'path:{path}')\n",
    "        signal = signal.cpu()\n",
    "        print(signal.shape)\n",
    "        \n",
    "        plt.figure(figsize=(16, 8), facecolor=\"white\")\n",
    "        plt.imshow(signal[0,:,:], origin='lower')\n",
    "        plt.autoscale(False)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.colorbar()\n",
    "        plt.axis('auto')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T03:34:57.294862Z",
     "iopub.status.busy": "2022-04-25T03:34:57.294037Z",
     "iopub.status.idle": "2022-04-25T03:34:57.30296Z",
     "shell.execute_reply": "2022-04-25T03:34:57.302098Z",
     "shell.execute_reply.started": "2022-04-25T03:34:57.294807Z"
    }
   },
   "outputs": [],
   "source": [
    "content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
