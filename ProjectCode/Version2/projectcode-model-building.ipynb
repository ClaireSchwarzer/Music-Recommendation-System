{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# install torchsummary\n","!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple torchsummary"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.tensorboard import SummaryWriter\n","from torch.utils.data import Dataset\n","import torchaudio\n","from torch.utils.data import DataLoader\n","import pandas as pd\n","import os\n","from itertools import product\n","from collections import namedtuple\n","from collections import OrderedDict\n","from IPython.display import display,clear_output\n","import time\n","import json\n","from torchsummary import summary\n","import matplotlib.pyplot as plt\n","\n","torch.set_printoptions(linewidth=120)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Add a column to the alphabetical list of label styles in the label file in numeric format\n","# Test dataset\n","ANNOTATIONS_FILE = \"/kaggle/input/projectdataset/GTZAN_TEST/GTZAN_TEST/features_30_sec_test.csv\"\n","dataframe = pd.read_csv(ANNOTATIONS_FILE)\n","\n","labels = set()\n","for row in range(len(dataframe)):\n","    labels.add(dataframe.iloc[row, -1])\n","labels_list = []\n","for label in labels:\n","    labels_list.append(label)\n","sorted_labels = sorted(labels_list)\n","sorted_labels\n","mapping = {}\n","for index, label in enumerate(sorted_labels):\n","    mapping[label] = index\n","dataframe[\"num_label\"] = dataframe[\"label\"]\n","new_dataframe = dataframe.replace({\"num_label\": mapping})\n","new_dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["new_dataframe.to_csv(\"features_30_sec_test_final.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Training dataset\n","import pandas as pd\n","ANNOTATIONS_FILE = \"/kaggle/input/projectdataset/GTZAN/GTZAN/features_30_sec.csv\"\n","dataframe = pd.read_csv(ANNOTATIONS_FILE)\n","labels = set()\n","for row in range(len(dataframe)):\n","    labels.add(dataframe.iloc[row, -1])\n","labels_list = []\n","for label in labels:\n","    labels_list.append(label)\n","sorted_labels = sorted(labels_list)\n","mapping = {}\n","for index, label in enumerate(sorted_labels):\n","    mapping[label] = index\n","dataframe[\"num_label\"] = dataframe[\"label\"]\n","new_dataframe = dataframe.replace({\"num_label\": mapping})\n","new_dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["new_dataframe.to_csv(\"features_30_sec_final.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# RunBuild class to manage hyperparameters, which can be automatically combined \n","# during the training process for predefined hyperparameters\n","class RunBuilder():\n","    @staticmethod\n","    def get_runs(params):\n","        Run = namedtuple('Run', params.keys())\n","        \n","        runs = []\n","        \n","        for element in product(*params.values()):\n","            runs.append(Run(*element))\n","        \n","        return runs"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Runtime data management classes\n","class RunManager():\n","    def __init__(self):\n","        #Training set\n","        # Number of epoches\n","        self.epoch_count = 0\n","        # Loss value per epoch\n","        self.epoch_loss = 0\n","        # Number of correct predictions per epoch\n","        self.epoch_correct_num = 0\n","        # Start time of training per epoch\n","        self.epoch_start_time = None\n","        \n","        # Test dataset\n","        self.test_epoch_count = 0\n","        self.test_epoch_loss = 0\n","        self.test_epoch_correct_num = 0\n","        \n","        \n","        # Hyperparameters of each run, number of cycles, etc.\n","        self.run_params = None\n","        self.run_count = 0\n","        self.run_data = []\n","        self.run_start_time = None\n","        \n","        \n","        self.network = None\n","        self.loader = None\n","        # tensorboard\n","        self.tb = None\n","    \n","    def begin_run(self, run, network, loader, test_loader):\n","        # Initial start time\n","        self.run_start_time = time.time()\n","        # Initialising hyperparameters\n","        self.run_params = run\n","        # run times +1\n","        self.run_count += 1\n","        \n","        self.network = network\n","        self.loader = loader\n","        self.test_loader = test_loader\n","        # Load tensorboard\n","        self.tb = SummaryWriter(comment=f'-{run}')\n","        \n","        # signal: sampling signal sr: sampling frequency\n","        signal, sr, address = next(iter(self.loader))\n","        \n","        \n","        # Signal conversion to mel-spectrum is missing here, no image visualisation added yet\n","        \n","        # Neural network structure image visualisation\n","        self.tb.add_graph(\n","            self.network,\n","            signal.to(run.device)\n","        )\n","        \n","    def end_run(self):\n","        # Close tensorboard to write data\n","        self.tb.close()\n","        # Each epoch is re-counted again\n","        self.epoch_count = 0\n","        self.test_epoch_count = 0\n","        \n","    def begin_epoch(self):\n","        self.epoch_start_time = time.time()\n","        self.epoch_count += 1\n","        self.epoch_loss = 0\n","        self.epoch_correct_num = 0\n","        \n","        self.test_epoch_count += 1\n","        self.test_epoch_loss = 0\n","        self.test_epoch_correct_num = 0\n","        \n","    def end_epoch(self):\n","        epoch_duration = time.time() - self.epoch_start_time\n","        run_duration = time.time() - self.run_start_time\n","\n","        # Training set loss values\n","        loss = self.epoch_loss / len(self.loader.dataset)\n","        # Test set Accuracy Rate\n","        accuracy = self.epoch_correct_num / len(self.loader.dataset)\n","        print(f'Accuracy Rateï¼š{self.epoch_correct_num} / {len(self.loader.dataset)}')\n","        \n","        # Test set\n","        # print(f\"{self.test_epoch_correct_num}+{len(self.test_loader.dataset)}\")\n","        test_loss = self.test_epoch_loss / len(self.test_loader.dataset)\n","        test_accuracy = self.test_epoch_correct_num / len(self.test_loader.dataset)\n","        \n","        # Add the loss function image\n","        self.tb.add_scalars('Loss', {\"train_loss\": loss, \n","                                    \"test_loss\": test_loss}, self.epoch_count)\n","        # Add an image of the accuracy function\n","        self.tb.add_scalars('Accuracy', {\"train_accuracy\": accuracy, \n","                                        \"test_accuracy\": test_accuracy}, self.epoch_count)\n","        \n","        # self.tb.add_scalar('Test_Loss', test_loss, self.epoch_count)\n","        \n","        #self.tb.add_scalar('Test_Accuracy', test_accuracy, self.epoch_count)\n","        \n","        for name, param in self.network.named_parameters():\n","            # The value of each layer of the neural network\n","            self.tb.add_histogram(name, param, self.epoch_count)\n","            # Gradient corresponding to each layer value\n","            self.tb.add_histogram(f'{name}.grad', param.grad, self.epoch_count)\n","\n","        results = OrderedDict()\n","\n","        results['run'] = self.run_count\n","        results['epoch'] = self.epoch_count\n","        results['loss'] = loss\n","        results['accuracy'] = accuracy\n","        results['epoch duration'] = epoch_duration\n","        results['run duration'] = run_duration\n","\n","        for k, v in self.run_params._asdict().items():\n","            results[k] = v\n","\n","        self.run_data.append(results)\n","\n","        df = pd.DataFrame.from_dict(self.run_data, orient='columns')\n","\n","        clear_output(wait = True)\n","        display(df)\n","        \n","    # def test_view(self):\n","        \n","    # Core number\n","    def get_num_workers(self,num_workers):\n","        self.epoch_num_workers = num_workers\n","\n","    # Training set \n","    # Record the loss of each epoch. Training set    \n","    def track_loss(self,loss,batch):\n","        self.epoch_loss += loss.item()*batch[0].shape[0]\n","    \n","    # Test set\n","    def test_loss(self,test_loss, test_batch):\n","         self.test_epoch_loss += test_loss.item()*test_batch[0].shape[0]\n","    \n","    # Record the number of correct tests on each epoch\n","    def test_num_correct(self, test_preds, test_labels):\n","        \n","        self.test_epoch_correct_num += self.get_correct_num(test_preds, test_labels)\n","        \n","    # Training set\n","    def track_num_correct(self, preds, labels):\n","        self.epoch_correct_num += self.get_correct_num(preds, labels)\n","    \n","    def get_correct_num(self, preds, labels):\n","        return preds.argmax(dim=1).eq(labels).sum().item()\n","    \n","    # Training data saved in CSV file\n","    def save(self, fileName):\n","        pd.DataFrame.from_dict(\n","            self.run_data, orient='columns'\n","        ).to_csv(f'{fileName}.csv')\n","        \n","        with open(f'{fileName}.json', 'w', encoding='utf-8') as f:\n","            json.dump(self.run_data, f, ensure_ascii=False, indent=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Data pre-processing classes\n","\n","class GTZANDataset(Dataset):\n","    def __init__(self,\n","                 annotations_file,\n","                 audio_dir,\n","                 transformation,\n","                 target_sample_rate,\n","                 num_samples,\n","                 device):\n","        # Read the label file\n","        self.annotations = pd.read_csv(annotations_file)\n","        # Reading audio addresses\n","        self.audio_dir = audio_dir\n","        # Set the device\n","        self.device = device\n","        # loaded into the deviceLoading Mel spectrum data into the device\n","        self.transformation = transformation.to(self.device)\n","        # Setting sampling frequency\n","        self.target_sample_rate = target_sample_rate\n","        # Set number of samples\n","        self.num_samples = num_samples\n","        \n","        \n","    # Returns the number of audio files\n","    def __len__(self):\n","        return len(self.annotations)\n","\n","    \n","    # Get data, tags, paths for audio\n","    def __getitem__(self, index):\n","        # Get the song path\n","        audio_sample_path = self._get_audio_sample_path(index)\n","        # Get the label\n","        label = self._get_audio_sample_label(index)\n","        # signal: sampling signal sr: sampling frequency\n","        signal, sr = torchaudio.load(audio_sample_path)\n","        signal = signal.to(self.device)\n","        # Control sampling frequency\n","        signal = self._resample_if_necessary(signal, sr)\n","        # Dual channel->single channel\n","        signal = self._mix_down_if_necessary(signal)\n","        # Control the number of samples\n","        signal = self._cut_if_necessary(signal)\n","        signal = self._right_pad_if_necessary(signal)\n","        # Transforming the mel spectrum\n","        signal = self.transformation(signal)\n","        return signal, label, audio_sample_path\n","\n","    \n","    # Whether the signal needs to be cropped. \n","    # If the number of picks > the set number -> crop.\n","    def _cut_if_necessary(self, signal):\n","        # print('_cut_if_necessary')\n","        if signal.shape[1] > self.num_samples:\n","            signal = signal[:, :self.num_samples]\n","        return signal\n","    \n","    \n","    # Whether the signal needs to be replenished. Fill in 0 to the right to replenish,\n","    # If the number of picks < the set number -> replenish\n","    def _right_pad_if_necessary(self, signal):\n","        length_signal = signal.shape[1]\n","        # print('_right_pad_if_necessary')\n","        if length_signal < self.num_samples:\n","            \n","            num_missing_samples = self.num_samples - length_signal\n","            last_dim_padding = (0, num_missing_samples)\n","            # last_dim_padding.to(self.device)\n","            \n","            signal = torch.nn.functional.pad(signal, last_dim_padding)\n","\n","        return signal\n","\n","    \n","    # Resetting the sampling frequency\n","    def _resample_if_necessary(self, signal, sr):\n","        # print('_resample_if_necessary')\n","        # If the actual sampling frequency does not match the setting -> reset it\n","        if sr != self.target_sample_rate:\n","            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate).to(self.device)\n","            signal = resampler(signal)\n","            # signal = torchaudio.functional.resample(signal, sr, self.target_sample_rate)\n","            \n","        return signal\n","\n","\n","    # Changing the audio from dual channel to single channel\n","    def _mix_down_if_necessary(self, signal):\n","        # print('_mix_down_if_necessary')\n","        \n","        # If the number of channels is greater than 1 ->\n","        # take the average value and turn it into a single channel\n","        if signal.shape[0] > 1:\n","            signal = torch.mean(signal, dim=0, keepdim=True)\n","        return signal\n","\n","    # Splicing and extraction of audio paths\n","    def _get_audio_sample_path(self, index):\n","        # print('_get_audio_sample_path')\n","        fold = f\"{self.annotations.iloc[index, -2]}\"\n","        path = os.path.join(self.audio_dir, fold, self.annotations.iloc[\n","            index, 1])\n","        return path\n","    \n","    \n","    # Extracting labels from csv files\n","    def _get_audio_sample_label(self, index):\n","        # print('_get_audio_sample_label')\n","        return self.annotations.iloc[index, -1]\n","    \n","\n","if __name__ == \"__main__\":\n","    ANNOTATIONS_FILE = \"./features_30_sec_final.csv\"\n","    AUDIO_DIR = \"/kaggle/input/projectdataset/GTZAN/GTZAN/genres_original\"\n","    SAMPLE_RATE = 22050\n","    NUM_SAMPLES = 22050 * 5 # -> 1 second of audio\n","    plot = True\n","\n","    if torch.cuda.is_available():\n","        device = \"cuda\"\n","    else:\n","        device = \"cpu\"\n","    print(f\"Using {device} device\")\n","\n","    mfcc = torchaudio.transforms.MFCC(\n","        sample_rate=SAMPLE_RATE,\n","        n_mfcc=40,\n","        log_mels=True\n","    )\n","\n","    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n","        sample_rate=SAMPLE_RATE,\n","        n_fft=1024,\n","        # Windows size\n","        hop_length=512,\n","        # Mel Frequency\n","        n_mels=64\n","    )\n","\n","    # objects inside transforms module are callable!\n","    # ms = mel_spectrogram(signal)\n","\n","    gtzan = GTZANDataset(\n","        ANNOTATIONS_FILE,\n","        AUDIO_DIR,\n","        mfcc,\n","        SAMPLE_RATE,\n","        NUM_SAMPLES,\n","        device\n","    )\n","\n","    print(f\"There are {len(gtzan)} samples in the dataset\")\n","\n","    if plot:\n","        signal, label, path = gtzan[666]\n","        print(f'path:{path}')\n","        signal = signal.cpu()\n","        print(signal.shape)\n","        \n","        plt.figure(figsize=(16, 8), facecolor=\"white\")\n","        plt.imshow(signal[0,:,:], origin='lower')\n","        plt.autoscale(False)\n","        plt.xlabel(\"Time\")\n","        plt.ylabel(\"Frequency\")\n","        plt.colorbar()\n","        plt.axis('auto')\n","        plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ANNOTATIONS_FILE = \"./features_30_sec_final.csv\"\n","AUDIO_DIR = \"/kaggle/input/projectdataset/GTZAN/GTZAN/genres_original\"\n","SAMPLE_RATE = 22050\n","NUM_SAMPLES = 22050  * 5\n","\n","# These next three functions don't actually do anything later and can be removed\n","# Creating a data loading set\n","def create_data_loader(train_data, batch_size):\n","    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True,num_workers=0, pin_memory=True)\n","    return train_dataloader\n","\n","\n","# Training for each epoch\n","def train_single_epoch(model, data_loader, loss_fn, optimiser, device):\n","    for input, target in data_loader:\n","        input, target = input.to(device), target.to(device)\n","\n","        # calculate loss\n","        prediction = model(input)\n","        loss = loss_fn(prediction, target)\n","\n","        # backpropagate error and update weights\n","        optimiser.zero_grad()\n","        loss.backward()\n","        optimiser.step()\n","\n","    print(f\"loss: {loss.item()}\")\n","\n","# Training\n","def train(model, data_loader, loss_fn, optimiser, device, epochs):\n","    for i in range(epochs):\n","        print(f\"Epoch {i+1}\")\n","        train_single_epoch(model, data_loader, loss_fn, optimiser, device)\n","        print(\"---------------------------\")\n","    print(\"Finished training\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# AlexNet network\n","class AlexNet(nn.Module):\n","    def __init__(self, num_classes=10):\n","        super(AlexNet, self).__init__()\n","        self.features = nn.Sequential(\n","            # Convolution\n","            # Input channel 1, output channel 64 Convolution kernel size 11*11 \n","            # Step size 4 Zero padding 2\n","            nn.Conv2d(1, 64, kernel_size=11, stride=4, padding=2),\n","            # ReLU activation function\n","            nn.ReLU(inplace=True),\n","            # Maximum pooling\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","        )\n","        # Flat\n","        self.flatten = nn.Flatten()\n","        # Classifier\n","        self.classifier = nn.Sequential(\n","            # Linear classifier Fully connected layer\n","            nn.Linear(12288, 1024),\n","            nn.ReLU(inplace=True),\n","            # Dropout Random inactivation\n","            nn.Dropout(p=0.5, inplace=False),\n","            nn.Linear(1024, 1024),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(p=0.3, inplace=False),\n","            nn.Linear(1024, num_classes),\n","        )\n","    # Forward transmission\n","    def forward(self, x):\n","        x = self.features(x)\n","        #x = x.view(-1, 3072)\n","        x = self.flatten(x)\n","        x = self.classifier(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if __name__ == '__main__':\n","    from torchsummary import summary\n","    alex=AlexNet().to(\"cuda\")\n","    summary(alex, (1, 128, 111* 5))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["torch.manual_seed(128)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Defining the dictionary of hyperparameters\n","params = OrderedDict(\n","    lr = [.001, .0001]\n","    , batch_size = [64]\n","    , num_workers = [0]\n","    , device = ['cuda']\n","    \n",")\n","\n","# Training set label file address\n","ANNOTATIONS_FILE = \"./features_30_sec_final.csv\"\n","# Training set audio file address\n","AUDIO_DIR = \"/kaggle/input/projectdataset/GTZAN/GTZAN/genres_original\"\n","\n","# Test set\n","ANNOTATIONS_FILE_TEST = \"./features_30_sec_test_final.csv\"\n","AUDIO_DIR_TEST = \"/kaggle/input/projectdataset/GTZAN_TEST/GTZAN_TEST/genres_original\""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
