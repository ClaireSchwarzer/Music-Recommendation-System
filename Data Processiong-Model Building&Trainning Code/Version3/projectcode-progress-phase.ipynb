{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# install torchsummary\n!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple torchsummary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data import Dataset\nimport torchaudio\nfrom torch.utils.data import DataLoader\nimport pandas as pd\nimport os\nfrom itertools import product\nfrom collections import namedtuple\nfrom collections import OrderedDict\nfrom IPython.display import display,clear_output\nimport time\nimport json\nfrom torchsummary import summary\nimport matplotlib.pyplot as plt\n\ntorch.set_printoptions(linewidth=120)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add a column to the alphabetical list of label styles in the label file in numeric format\n# Test dataset\nANNOTATIONS_FILE = \"/kaggle/input/projectdataset/GTZAN_TEST/GTZAN_TEST/features_30_sec_test.csv\"\ndataframe = pd.read_csv(ANNOTATIONS_FILE)\n\nlabels = set()\nfor row in range(len(dataframe)):\n    labels.add(dataframe.iloc[row, -1])\nlabels_list = []\nfor label in labels:\n    labels_list.append(label)\nsorted_labels = sorted(labels_list)\nsorted_labels\nmapping = {}\nfor index, label in enumerate(sorted_labels):\n    mapping[label] = index\ndataframe[\"num_label\"] = dataframe[\"label\"]\nnew_dataframe = dataframe.replace({\"num_label\": mapping})\nnew_dataframe","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_dataframe.to_csv(\"features_30_sec_test_final.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training dataset\nimport pandas as pd\nANNOTATIONS_FILE = \"/kaggle/input/projectdataset/GTZAN/GTZAN/features_30_sec.csv\"\ndataframe = pd.read_csv(ANNOTATIONS_FILE)\nlabels = set()\nfor row in range(len(dataframe)):\n    labels.add(dataframe.iloc[row, -1])\nlabels_list = []\nfor label in labels:\n    labels_list.append(label)\nsorted_labels = sorted(labels_list)\nmapping = {}\nfor index, label in enumerate(sorted_labels):\n    mapping[label] = index\ndataframe[\"num_label\"] = dataframe[\"label\"]\nnew_dataframe = dataframe.replace({\"num_label\": mapping})\nnew_dataframe","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_dataframe.to_csv(\"features_30_sec_final.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RunBuild class to manage hyperparameters, which can be automatically combined \n# during the training process for predefined hyperparameters\nclass RunBuilder():\n    @staticmethod\n    def get_runs(params):\n        Run = namedtuple('Run', params.keys())\n        \n        runs = []\n        \n        for element in product(*params.values()):\n            runs.append(Run(*element))\n        \n        return runs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Runtime data management classes\nclass RunManager():\n    def __init__(self):\n        #Training set\n        # Number of epoches\n        self.epoch_count = 0\n        # Loss value per epoch\n        self.epoch_loss = 0\n        # Number of correct predictions per epoch\n        self.epoch_correct_num = 0\n        # Start time of training per epoch\n        self.epoch_start_time = None\n        \n        # Test dataset\n        self.test_epoch_count = 0\n        self.test_epoch_loss = 0\n        self.test_epoch_correct_num = 0\n        \n        \n        # Hyperparameters of each run, number of cycles, etc.\n        self.run_params = None\n        self.run_count = 0\n        self.run_data = []\n        self.run_start_time = None\n        \n        \n        self.network = None\n        self.loader = None\n        # tensorboard\n        self.tb = None\n    \n    def begin_run(self, run, network, loader, test_loader):\n        # Initial start time\n        self.run_start_time = time.time()\n        # Initialising hyperparameters\n        self.run_params = run\n        # run times +1\n        self.run_count += 1\n        \n        self.network = network\n        self.loader = loader\n        self.test_loader = test_loader\n        # Load tensorboard\n        self.tb = SummaryWriter(comment=f'-{run}')\n        \n        # signal: sampling signal sr: sampling frequency\n        signal, sr, address = next(iter(self.loader))\n        \n        \n        # Signal conversion to mel-spectrum is missing here, no image visualisation added yet\n        \n        # Neural network structure image visualisation\n        self.tb.add_graph(\n            self.network,\n            signal.to(run.device)\n        )\n        \n    def end_run(self):\n        # Close tensorboard to write data\n        self.tb.close()\n        # Each epoch is re-counted again\n        self.epoch_count = 0\n        self.test_epoch_count = 0\n        \n    def begin_epoch(self):\n        self.epoch_start_time = time.time()\n        self.epoch_count += 1\n        self.epoch_loss = 0\n        self.epoch_correct_num = 0\n        \n        self.test_epoch_count += 1\n        self.test_epoch_loss = 0\n        self.test_epoch_correct_num = 0\n        \n    def end_epoch(self):\n        epoch_duration = time.time() - self.epoch_start_time\n        run_duration = time.time() - self.run_start_time\n\n        # Training set loss values\n        loss = self.epoch_loss / len(self.loader.dataset)\n        # Test set Accuracy Rate\n        accuracy = self.epoch_correct_num / len(self.loader.dataset)\n        print(f'Accuracy Rate：{self.epoch_correct_num} / {len(self.loader.dataset)}')\n        \n        # Test set\n        # print(f\"{self.test_epoch_correct_num}+{len(self.test_loader.dataset)}\")\n        test_loss = self.test_epoch_loss / len(self.test_loader.dataset)\n        test_accuracy = self.test_epoch_correct_num / len(self.test_loader.dataset)\n        \n        # Add the loss function image\n        self.tb.add_scalars('Loss', {\"train_loss\": loss, \n                                    \"test_loss\": test_loss}, self.epoch_count)\n        # Add an image of the accuracy function\n        self.tb.add_scalars('Accuracy', {\"train_accuracy\": accuracy, \n                                        \"test_accuracy\": test_accuracy}, self.epoch_count)\n        \n        # self.tb.add_scalar('Test_Loss', test_loss, self.epoch_count)\n        \n        #self.tb.add_scalar('Test_Accuracy', test_accuracy, self.epoch_count)\n        \n        for name, param in self.network.named_parameters():\n            # The value of each layer of the neural network\n            self.tb.add_histogram(name, param, self.epoch_count)\n            # Gradient corresponding to each layer value\n            self.tb.add_histogram(f'{name}.grad', param.grad, self.epoch_count)\n\n        results = OrderedDict()\n\n        results['run'] = self.run_count\n        results['epoch'] = self.epoch_count\n        results['loss'] = loss\n        results['accuracy'] = accuracy\n        results['epoch duration'] = epoch_duration\n        results['run duration'] = run_duration\n\n        for k, v in self.run_params._asdict().items():\n            results[k] = v\n\n        self.run_data.append(results)\n\n        df = pd.DataFrame.from_dict(self.run_data, orient='columns')\n\n        clear_output(wait = True)\n        display(df)\n        \n    # def test_view(self):\n        \n    # Core number\n    def get_num_workers(self,num_workers):\n        self.epoch_num_workers = num_workers\n\n    # Training set \n    # Record the loss of each epoch. Training set    \n    def track_loss(self,loss,batch):\n        self.epoch_loss += loss.item()*batch[0].shape[0]\n    \n    # Test set\n    def test_loss(self,test_loss, test_batch):\n         self.test_epoch_loss += test_loss.item()*test_batch[0].shape[0]\n    \n    # Record the number of correct tests on each epoch\n    def test_num_correct(self, test_preds, test_labels):\n        \n        self.test_epoch_correct_num += self.get_correct_num(test_preds, test_labels)\n        \n    # Training set\n    def track_num_correct(self, preds, labels):\n        self.epoch_correct_num += self.get_correct_num(preds, labels)\n    \n    def get_correct_num(self, preds, labels):\n        return preds.argmax(dim=1).eq(labels).sum().item()\n    \n    # Training data saved in CSV file\n    def save(self, fileName):\n        pd.DataFrame.from_dict(\n            self.run_data, orient='columns'\n        ).to_csv(f'{fileName}.csv')\n        \n        with open(f'{fileName}.json', 'w', encoding='utf-8') as f:\n            json.dump(self.run_data, f, ensure_ascii=False, indent=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data pre-processing classes\n\nclass GTZANDataset(Dataset):\n    def __init__(self,\n                 annotations_file,\n                 audio_dir,\n                 transformation,\n                 target_sample_rate,\n                 num_samples,\n                 device):\n        # Read the label file\n        self.annotations = pd.read_csv(annotations_file)\n        # Reading audio addresses\n        self.audio_dir = audio_dir\n        # Set the device\n        self.device = device\n        # loaded into the deviceLoading Mel spectrum data into the device\n        self.transformation = transformation.to(self.device)\n        # Setting sampling frequency\n        self.target_sample_rate = target_sample_rate\n        # Set number of samples\n        self.num_samples = num_samples\n        \n        \n    # Returns the number of audio files\n    def __len__(self):\n        return len(self.annotations)\n\n    \n    # Get data, tags, paths for audio\n    def __getitem__(self, index):\n        # Get the song path\n        audio_sample_path = self._get_audio_sample_path(index)\n        # Get the label\n        label = self._get_audio_sample_label(index)\n        # signal: sampling signal sr: sampling frequency\n        signal, sr = torchaudio.load(audio_sample_path)\n        signal = signal.to(self.device)\n        # Control sampling frequency\n        signal = self._resample_if_necessary(signal, sr)\n        # Dual channel->single channel\n        signal = self._mix_down_if_necessary(signal)\n        # Control the number of samples\n        signal = self._cut_if_necessary(signal)\n        signal = self._right_pad_if_necessary(signal)\n        # Transforming the mel spectrum\n        signal = self.transformation(signal)\n        return signal, label, audio_sample_path\n\n    \n    # Whether the signal needs to be cropped. \n    # If the number of picks > the set number -> crop.\n    def _cut_if_necessary(self, signal):\n        # print('_cut_if_necessary')\n        if signal.shape[1] > self.num_samples:\n            signal = signal[:, :self.num_samples]\n        return signal\n    \n    \n    # Whether the signal needs to be replenished. Fill in 0 to the right to replenish,\n    # If the number of picks < the set number -> replenish\n    def _right_pad_if_necessary(self, signal):\n        length_signal = signal.shape[1]\n        # print('_right_pad_if_necessary')\n        if length_signal < self.num_samples:\n            \n            num_missing_samples = self.num_samples - length_signal\n            last_dim_padding = (0, num_missing_samples)\n            # last_dim_padding.to(self.device)\n            \n            signal = torch.nn.functional.pad(signal, last_dim_padding)\n\n        return signal\n\n    \n    # Resetting the sampling frequency\n    def _resample_if_necessary(self, signal, sr):\n        # print('_resample_if_necessary')\n        # If the actual sampling frequency does not match the setting -> reset it\n        if sr != self.target_sample_rate:\n            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate).to(self.device)\n            signal = resampler(signal)\n            # signal = torchaudio.functional.resample(signal, sr, self.target_sample_rate)\n            \n        return signal\n\n\n    # Changing the audio from dual channel to single channel\n    def _mix_down_if_necessary(self, signal):\n        # print('_mix_down_if_necessary')\n        \n        # If the number of channels is greater than 1 ->\n        # take the average value and turn it into a single channel\n        if signal.shape[0] > 1:\n            signal = torch.mean(signal, dim=0, keepdim=True)\n        return signal\n\n    # Splicing and extraction of audio paths\n    def _get_audio_sample_path(self, index):\n        # print('_get_audio_sample_path')\n        fold = f\"{self.annotations.iloc[index, -2]}\"\n        path = os.path.join(self.audio_dir, fold, self.annotations.iloc[\n            index, 1])\n        return path\n    \n    \n    # Extracting labels from csv files\n    def _get_audio_sample_label(self, index):\n        # print('_get_audio_sample_label')\n        return self.annotations.iloc[index, -1]\n    \n\nif __name__ == \"__main__\":\n    ANNOTATIONS_FILE = \"./features_30_sec_final.csv\"\n    AUDIO_DIR = \"/kaggle/input/projectdataset/GTZAN/GTZAN/genres_original\"\n    SAMPLE_RATE = 22050\n    NUM_SAMPLES = 22050 * 5 # -> 1 second of audio\n    plot = True\n\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    else:\n        device = \"cpu\"\n    print(f\"Using {device} device\")\n\n    mfcc = torchaudio.transforms.MFCC(\n        sample_rate=SAMPLE_RATE,\n        n_mfcc=40,\n        log_mels=True\n    )\n\n    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n        sample_rate=SAMPLE_RATE,\n        n_fft=1024,\n        # Windows size\n        hop_length=512,\n        # Mel Frequency\n        n_mels=64\n    )\n\n    # objects inside transforms module are callable!\n    # ms = mel_spectrogram(signal)\n\n    gtzan = GTZANDataset(\n        ANNOTATIONS_FILE,\n        AUDIO_DIR,\n        mfcc,\n        SAMPLE_RATE,\n        NUM_SAMPLES,\n        device\n    )\n\n    print(f\"There are {len(gtzan)} samples in the dataset\")\n\n    if plot:\n        signal, label, path = gtzan[666]\n        print(f'path:{path}')\n        signal = signal.cpu()\n        print(signal.shape)\n        \n        plt.figure(figsize=(16, 8), facecolor=\"white\")\n        plt.imshow(signal[0,:,:], origin='lower')\n        plt.autoscale(False)\n        plt.xlabel(\"Time\")\n        plt.ylabel(\"Frequency\")\n        plt.colorbar()\n        plt.axis('auto')\n        plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ANNOTATIONS_FILE = \"./features_30_sec_final.csv\"\nAUDIO_DIR = \"/kaggle/input/projectdataset/GTZAN/GTZAN/genres_original\"\nSAMPLE_RATE = 22050\nNUM_SAMPLES = 22050  * 5\n\n# These next three functions don't actually do anything later and can be removed\n# Creating a data loading set\ndef create_data_loader(train_data, batch_size):\n    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True,num_workers=0, pin_memory=True)\n    return train_dataloader\n\n\n# Training for each epoch\ndef train_single_epoch(model, data_loader, loss_fn, optimiser, device):\n    for input, target in data_loader:\n        input, target = input.to(device), target.to(device)\n\n        # calculate loss\n        prediction = model(input)\n        loss = loss_fn(prediction, target)\n\n        # backpropagate error and update weights\n        optimiser.zero_grad()\n        loss.backward()\n        optimiser.step()\n\n    print(f\"loss: {loss.item()}\")\n\n# Training\ndef train(model, data_loader, loss_fn, optimiser, device, epochs):\n    for i in range(epochs):\n        print(f\"Epoch {i+1}\")\n        train_single_epoch(model, data_loader, loss_fn, optimiser, device)\n        print(\"---------------------------\")\n    print(\"Finished training\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# AlexNet network\nclass AlexNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super(AlexNet, self).__init__()\n        self.features = nn.Sequential(\n            # Convolution\n            # Input channel 1, output channel 64 Convolution kernel size 11*11 \n            # Step size 4 Zero padding 2\n            nn.Conv2d(1, 64, kernel_size=11, stride=4, padding=2),\n            # ReLU activation function\n            nn.ReLU(inplace=True),\n            # Maximum pooling\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        # Flat\n        self.flatten = nn.Flatten()\n        # Classifier\n        self.classifier = nn.Sequential(\n            # Linear classifier Fully connected layer\n            nn.Linear(12288, 1024),\n            nn.ReLU(inplace=True),\n            # Dropout Random inactivation\n            nn.Dropout(p=0.5, inplace=False),\n            nn.Linear(1024, 1024),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.3, inplace=False),\n            nn.Linear(1024, num_classes),\n        )\n    # Forward transmission\n    def forward(self, x):\n        x = self.features(x)\n        #x = x.view(-1, 3072)\n        x = self.flatten(x)\n        x = self.classifier(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    from torchsummary import summary\n    alex=AlexNet().to(\"cuda\")\n    summary(alex, (1, 128, 111* 5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(128)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining the dictionary of hyperparameters\nparams = OrderedDict(\n    lr = [.001, .0001]\n    , batch_size = [64]\n    , num_workers = [0]\n    , device = ['cuda']\n    \n)\n\n# Training set label file address\nANNOTATIONS_FILE = \"./features_30_sec_final.csv\"\n# Training set audio file address\nAUDIO_DIR = \"/kaggle/input/projectdataset/GTZAN/GTZAN/genres_original\"\n\n# Test set\nANNOTATIONS_FILE_TEST = \"./features_30_sec_test_final.csv\"\nAUDIO_DIR_TEST = \"/kaggle/input/projectdataset/GTZAN_TEST/GTZAN_TEST/genres_original\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Did not use this.\nmfcc = torchaudio.transforms.MFCC(\n        sample_rate=SAMPLE_RATE,\n        n_mfcc=128,\n        log_mels=True\n)\n\n# Mel spectrum conversion\nmel_spectrogram = torchaudio.transforms.MelSpectrogram(\n    sample_rate=SAMPLE_RATE,\n    n_fft=1024,\n    hop_length=512,\n    n_mels=64\n)\n\n\n\n\n\nm = RunManager()\n# c = 0\n# acc = 0\n# now_acc = 0\n# Here's the real training\nfor run in RunBuilder.get_runs(params):\n    usd = GTZANDataset(ANNOTATIONS_FILE,\n                        AUDIO_DIR,\n                        mfcc,\n                        SAMPLE_RATE,\n                        NUM_SAMPLES,\n                        run.device)\n    usd_test = GTZANDataset(\n        ANNOTATIONS_FILE_TEST,\n        AUDIO_DIR_TEST,\n        mfcc,\n        SAMPLE_RATE,\n        NUM_SAMPLES,\n        run.device\n    )\n    \n    print(run)\n    device = torch.device(run.device)\n    \n#     train_dataloader = create_data_loader(usd,\n#                                          batch_size = run.batch_size,\n#                                          num_workers = run.num_workers)\n    \n    train_data_loader = DataLoader(usd, batch_size=run.batch_size, num_workers = run.num_workers, shuffle=True)\n    \n    test_data_loader = DataLoader(usd_test, batch_size=run.batch_size,num_workers = run.num_workers)\n    \n    # network = VGG16().to(device)\n    # network = CNNNetwork().to(device)\n    # network = ANNNet().to(device)\n    network = AlexNet().to(device)\n    print(network)\n    # Optimizer\n    optimizer = optim.Adam(network.parameters(),lr=run.lr)\n    m.begin_run(run, network, train_data_loader, test_data_loader)\n    #best_loss Initialised to +∞.\n    best_loss = float('inf')\n    for epoch in range(100):\n        network.train()\n        m.begin_epoch()\n        for batch in train_data_loader:\n            input = batch[0].to(device)\n            target = batch[1].to(device)\n            preds = network(input)\n            loss = F.cross_entropy(preds,target)\n            optimizer.zero_grad()\n            # Reverse transmission\n            loss.backward()\n            optimizer.step()\n            m.track_loss(loss, batch)\n            m.track_num_correct(preds, target)\n                    \n        with torch.no_grad():\n            # This part is used for testing, so it does not calculate the gradient\n            for test_batch in test_data_loader:\n                test_input = batch[0].to(device)\n                test_target = batch[1].to(device)\n                test_preds = network(test_input)\n                test_loss = F.cross_entropy(test_preds,test_target)\n\n                m.test_loss(test_loss, test_batch)\n                \n                m.test_num_correct(test_preds, test_target)    \n        m.end_epoch()\n        \n    # Save the model   \n    torch.save(network.state_dict(), f'best_model.pth')\n    m.end_run()\n    m.save(f'{run.lr}_{run.batch_size}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r runs.zip runs\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!rm -rf runs\n\n#!rm -rf \"runs.zip\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_mapping = [\n    'blues',\n    'classical',\n    'country',\n    'disco',\n    'hiphop',\n    'jazz',\n    'metal',\n    'pop',\n    'reggae',\n    'rock'\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n\nran = random.sample(range(0,1000),10)\nran ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X: tensor of the input mel spectrum, y: actual label subscript, \n# class_mapping: label dictionary\ndef predict(model, X, y, class_mapping):\n    model.eval()    # train <-> eval: changes how model behave (e.g. no dropout, ...)\n    with torch.no_grad():\n        predictions = model(X)\n        # tensor (1, 10) -> [ [0.1, 0.04, ..., 0.6] ]\n        # Fetch the largest output subscript\n        predicted_index = predictions[0].argmax(0)\n        # Prediction labels\n        predicted = class_mapping[predicted_index]\n        # The actual label\n        expected = class_mapping[y]\n        \n    return predicted, expected","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test accuracy of test sets\ndef verify_acc(local):\n    \n    # load back the model\n    #cnn = CNNNetwork()\n    #cnn = VGG16()\n    cnn = AlexNet()\n    #cnn = ANNNet()\n    state_dict = torch.load(local)\n    cnn.load_state_dict(state_dict)\n\n    # load gtzan validation dataset\n    mfcc = torchaudio.transforms.MFCC(\n        sample_rate=SAMPLE_RATE,\n        n_mfcc=128,\n        log_mels=True\n    )\n\n    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n        sample_rate=SAMPLE_RATE,\n        n_fft=1024,\n        hop_length=512,\n        n_mels=64\n    )\n\n    gtzan = GTZANDataset(\n        annotations_file=ANNOTATIONS_FILE,\n        audio_dir=AUDIO_DIR,\n        transformation=mfcc,\n        target_sample_rate=SAMPLE_RATE,\n        num_samples=NUM_SAMPLES,\n        device=\"cpu\"\n    )\n\n    count = 0\n    for i in range(0,800):\n        index = i\n\n        # get a sample from the gtzan dataset for inference\n        X, y = gtzan[index][0], gtzan[index][1] # [batch_size, num_channels, freq, time]\n        X.unsqueeze_(0) # insert an extra dimension at index 0\n        #print(X.shape)\n        #print(y)\n\n        # make an inference\n        predicted, expected = predict(cnn, X, y, class_mapping)\n        #print(f\"Predicted: {predicted}\")\n        #print(f\"Expected: {expected}\")\n        if predicted == expected:\n            \n            count += 1\n            #print(count)\n    print(count/800.00)\n    return (count/800.00) \nacc = verify_acc(\"best_model.pth\")\nacc        \n#         print(f\"Predicted: {predicted}\")\n#         print(f\"Expected: {expected}\")\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\ncnn = AlexNet()\nstate_dict = torch.load(\"best_model.pth\")\ncnn.load_state_dict(state_dict)\nmfcc = torchaudio.transforms.MFCC(\n        sample_rate=SAMPLE_RATE,\n        n_mfcc=128,\n        log_mels=True\n)\n\nmel_spectrogram = torchaudio.transforms.MelSpectrogram(\n    sample_rate=SAMPLE_RATE,\n    n_fft=1024,\n    hop_length=512,\n    n_mels=64\n)\n\ngtzan = GTZANDataset(\n    annotations_file=ANNOTATIONS_FILE,\n    audio_dir=AUDIO_DIR,\n    transformation=mfcc,\n    target_sample_rate=SAMPLE_RATE,\n    num_samples=NUM_SAMPLES,\n    device=\"cpu\"\n)\n\n\n\n\ncnn.eval()\n\ninitial = random.sample(range(0,800),1)[0]\nprint(f'initial:{initial}')\n\nmusic_init_index = gtzan[initial][1]\nmusic_init_url = gtzan[initial][2]\n\nprint(f'music_init:{music_init_url}')\n\n\nran = random.sample(range(0,800),15)\n\nmax_music_value = - float(\"inf\")\nmax_music_index = None\nmax_music_url = None\nreal_label_index = None\n\ncontent={}\nfor i in range(15):\n    with torch.no_grad():\n        predictions = cnn(gtzan[ran[i]][0].unsqueeze_(0))\n        predicted_item = predictions[0][music_init_index].item()\n        if max_music_value < predicted_item:\n            max_music_value = predicted_item\n            max_music_index = i\n            real_label_index = gtzan[ran[i]][1]\n            max_music_url = gtzan[ran[i]][2]\n        content[gtzan[ran[i]][2]] = predicted_item\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_mapping[music_init_index]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"content","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_music_value","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_music_index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"real_label_index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_mapping[real_label_index]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_music_url","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ran = random.sample(range(0,1000),10)\nfor i in range(10):\n    print(ran[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}